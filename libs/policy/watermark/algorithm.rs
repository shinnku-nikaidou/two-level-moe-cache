//! Watermark algorithm implementation
//!
//! This module implements the dual watermark algorithm from the research documentation.
//! The algorithm manages a two-tier cache system (VRAM + RAM) for Mixture-of-Experts models
//! using adaptive watermark thresholds based on benefit density calculations.
//!
//! # Mathematical Foundation
//!
//! ## 1. Benefit Density Calculation
//! For each expert (e,ℓ) at time t, we calculate benefit densities for each tier:
//! - VRAM benefit: `b^G_{e,ℓ}(t) = p^{fuse}_{e,ℓ}(t) * C^G_{e,ℓ} / S_{e,ℓ}`
//! - RAM benefit:  `b^R_{e,ℓ}(t) = p^{fuse}_{e,ℓ}(t) * C^R_{e,ℓ} / S_{e,ℓ}`
//!
//! Where:
//! - `p^{fuse}_{e,ℓ}(t)`: Fused probability of expert activation
//! - `C^G_{e,ℓ}`, `C^R_{e,ℓ}`: Access costs for VRAM and RAM tiers
//! - `S_{e,ℓ}`: Expert size in bytes
//!
//! ## 2. Watermark Updates (Subgradient Method)
//! Watermarks evolve according to the dual optimization formulation:
//! - `λ_G ← [λ_G + η_G(usage_G - K_G)]_+`
//! - `λ_R ← [λ_R + η_R(usage_R - K_R)]_+`
//!
//! Where:
//! - `λ_G`, `λ_R`: Current watermark thresholds for VRAM and RAM
//! - `η_G`, `η_R`: Learning rates (step sizes) for watermark updates
//! - `usage_G`, `usage_R`: Current memory usage in bytes
//! - `K_G`, `K_R`: Capacity limits in bytes
//! - `[z]_+ := max(z, 0)`: Non-negative projection (ReLU)
//!
//! ## 3. Cache Decisions
//! Each expert is assigned to the highest tier where its benefit density exceeds the watermark:
//! - Place in VRAM if: `b^G_{e,ℓ} ≥ λ_G`
//! - Otherwise, place in RAM if: `b^R_{e,ℓ} ≥ λ_R`
//! - Otherwise, keep on Disk (NVMe storage)
//!
//! ## 4. Feasibility Repair Loop
//! The algorithm ensures capacity constraints through an iterative process:
//! 1. Make cache decisions using current watermarks
//! 2. Calculate resulting memory usage
//! 3. Apply subgradient updates to watermarks
//! 4. If constraints violated, repeat with updated watermarks
//! 5. Mathematical convergence guarantees feasible solution exists

use crate::constants::ModelType;
use serde::{Deserialize, Serialize};

/// Cache decision for an expert weight
///
/// Represents the possible cache management actions that can be taken
/// for an expert weight based on watermark algorithm decisions.
/// These decisions are generated by comparing benefit densities against watermarks.
#[derive(Debug, Clone, PartialEq)]
pub enum CacheDecision {
    /// Keep expert in VRAM (highest priority tier)
    KeepInVRAM,
    /// Demote expert from VRAM to RAM (benefit below VRAM watermark)
    DemoteToRAM,
    /// Keep expert in RAM (medium priority tier)
    KeepInRAM,
    /// Evict expert from RAM to disk (benefit below RAM watermark)
    EvictToDisk,
    /// Load expert to RAM from disk (benefit above RAM watermark)
    LoadToRAM,
    /// Promote expert to VRAM from RAM (benefit above VRAM watermark)
    PromoteToVRAM,
}

/// Memory tier hierarchy for expert residence
///
/// Represents the three-tier memory hierarchy used in the caching system.
/// The hierarchy enforces a containment constraint: if an expert is in tier i,
/// it must also be present in all slower tiers j > i.
///
/// Performance characteristics:
/// - VRAM: ~100 GB/s bandwidth, ~8-48GB capacity
/// - RAM: ~50-100 GB/s bandwidth, ~64-256GB capacity  
/// - Disk: ~3-7 GB/s bandwidth, ~1-8TB capacity
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum MemoryTier {
    /// GPU VRAM - fastest access, most limited capacity
    /// All experts in VRAM are also cached in RAM (containment constraint)
    Vram = 0,

    /// System RAM - fast access, moderate capacity
    /// Intermediate tier between VRAM and persistent storage
    Ram = 1,

    /// NVMe/SSD storage - slower access, largest capacity
    /// Base tier where all experts are permanently stored
    Disk = 2,
}

/// Expert state matrix representing cache decisions across all experts
///
/// This structure maintains the current memory tier assignment for every expert
/// in the MoE model. The state is organized as a 2D matrix:
/// `inner[layer_idx][expert_idx] -> MemoryTier`
///
/// # Memory Layout
/// For a model with L layers and E experts per layer:
/// - Layer 0: [expert_0_tier, expert_1_tier, ..., expert_{E-1}_tier]
/// - Layer 1: [expert_0_tier, expert_1_tier, ..., expert_{E-1}_tier]
/// - ...
/// - Layer L-1: [expert_0_tier, expert_1_tier, ..., expert_{E-1}_tier]
///
/// # Invariants
/// - All experts start in `MemoryTier::Disk` (default state)
/// - Containment constraint: VRAM ⊆ RAM ⊆ DISK (maintained by watermark decisions)
/// - State transitions are driven by benefit density vs watermark comparisons
pub struct ExpertState {
    /// 2D matrix: [layer_idx][expert_idx] -> MemoryTier
    /// Public for direct access in functional programming style
    pub inner: Vec<Vec<MemoryTier>>,
}

impl ExpertState {
    /// Create a new ExpertState with given dimensions
    ///
    /// All experts are initialized to `MemoryTier::Disk` (cold start).
    /// This represents the initial state where all expert weights are
    /// stored on persistent storage and no cache warming has occurred.
    ///
    /// # Arguments
    /// * `num_layers` - Number of transformer layers in the model
    /// * `num_experts` - Number of experts per layer
    pub fn new(num_layers: usize, num_experts: usize) -> Self {
        let inner = vec![vec![MemoryTier::Disk; num_experts]; num_layers];
        Self { inner }
    }

    /// Create ExpertState with model-specific dimensions
    ///
    /// Convenience constructor that extracts layer and expert counts
    /// from the model configuration automatically.
    pub fn with_model(model_type: ModelType) -> Self {
        let config: crate::constants::ModelConfig = model_type.into();
        Self::new(config.total_layers, config.experts_per_layer)
    }

    /// Get memory tier for a specific expert-layer pair
    ///
    /// # Arguments
    /// * `layer_id` - Transformer layer index (0-based)
    /// * `expert_id` - Expert index within the layer (0-based)
    ///
    /// # Returns
    /// Current memory tier assignment for the specified expert
    pub fn get(&self, layer_id: usize, expert_id: usize) -> MemoryTier {
        self.inner[layer_id][expert_id]
    }

    /// Set memory tier for a specific expert-layer pair
    ///
    /// Performs bounds checking before assignment to prevent panics.
    /// If indices are out of bounds, the operation is silently ignored.
    ///
    /// # Arguments
    /// * `layer_id` - Transformer layer index (0-based)
    /// * `expert_id` - Expert index within the layer (0-based)  
    /// * `tier` - Target memory tier for assignment
    pub fn set(&mut self, layer_id: usize, expert_id: usize, tier: MemoryTier) {
        if layer_id < self.inner.len() && expert_id < self.inner[layer_id].len() {
            self.inner[layer_id][expert_id] = tier;
        }
    }
}

/// Dual watermark algorithm for two-tier MoE expert caching
///
/// This structure implements the core watermark-based caching algorithm described
/// in the research documentation. It manages expert placement across a two-tier
/// memory hierarchy (VRAM + RAM) using adaptive watermark thresholds.
///
/// # Algorithm Overview
///
/// The algorithm operates in a feedback loop:
/// 1. **Benefit Calculation**: For each expert, compute value-per-byte ratios
/// 2. **Cache Decisions**: Compare benefits against current watermarks  
/// 3. **Usage Tracking**: Calculate memory consumption from decisions
/// 4. **Watermark Updates**: Apply subgradient method to adjust thresholds
/// 5. **Feasibility Repair**: Iterate until capacity constraints are satisfied
///
/// # Mathematical Foundations
///
/// ## Benefit Density
/// For expert (e,ℓ), the benefit density for tier T is:
/// `b^T_{e,ℓ} = p^{fuse}_{e,ℓ} × C^T_{e,ℓ} / S_{e,ℓ}`
///
/// Where:
/// - `p^{fuse}`: Fused probability from EWMA + ScoutGate predictors
/// - `C^T`: Cost of accessing tier T (loading latency)
/// - `S`: Expert size in bytes
///
/// ## Watermark Evolution
/// Watermarks evolve via projected subgradient descent:
/// `λ^T ← [λ^T + η^T × (usage^T - capacity^T)]₊`
///
/// This ensures watermarks rise when over-capacity and fall when under-capacity.
///
/// # Design Principles
///
/// - **Model-Agnostic**: Works with any MoE architecture
/// - **Online Learning**: Adapts to changing activation patterns
/// - **Capacity-Aware**: Respects hard memory constraints
/// - **Benefit-Driven**: Prioritizes high-value experts for faster tiers
pub struct WatermarkAlgorithm {
    /// VRAM capacity in bytes (K_G in mathematical notation)
    /// Represents the total GPU memory available for expert caching
    vram_capacity: usize,

    /// RAM capacity in bytes (K_R in mathematical notation)  
    /// Represents the total system memory available for expert caching
    ram_capacity: usize,

    /// VRAM watermark learning rate (η_G in mathematical notation)
    /// Controls the speed of watermark adaptation for GPU memory tier
    /// Higher values = faster adaptation, lower values = more stability
    vram_learning_rate: f64,

    /// RAM watermark learning rate (η_R in mathematical notation)
    /// Controls the speed of watermark adaptation for system memory tier  
    /// Typically set equal to vram_learning_rate for balanced behavior
    ram_learning_rate: f64,

    /// Cost of promoting expert from RAM to VRAM (C^G in mathematical notation)
    /// Represents the latency penalty of PCIe transfer + GPU memory allocation
    /// Units: arbitrary cost units (typically normalized)
    cost_g: f64,

    /// Cost of loading expert from NVMe to RAM (C^R in mathematical notation)  
    /// Represents the latency penalty of SSD I/O + memory allocation
    /// Should be higher than cost_g to reflect the storage hierarchy
    cost_r: f64,

    /// Expert size in bytes (S_{e,ℓ} in mathematical notation)
    /// Assumes all experts have uniform size for simplicity
    /// Real implementations may use per-expert sizes
    expert_size: usize,

    /// Current VRAM watermark threshold (λ_G in mathematical notation)
    /// Experts with benefit density ≥ this value are placed in VRAM
    /// Evolves dynamically based on capacity pressure
    vram_watermark: f64,

    /// Current RAM watermark threshold (λ_R in mathematical notation)
    /// Experts with benefit density ≥ this value are placed in RAM
    /// Evolves dynamically based on capacity pressure  
    ram_watermark: f64,
}

impl WatermarkAlgorithm {
    /// Create a new watermark algorithm instance with specified parameters
    ///
    /// This constructor allows full control over all algorithm parameters.
    /// Use `from_model()` for model-specific default configurations.
    ///
    /// # Arguments
    /// * `vram_capacity` - Total VRAM available for caching (bytes)
    /// * `ram_capacity` - Total RAM available for caching (bytes)  
    /// * `vram_learning_rate` - Watermark adaptation speed for VRAM (η_G)
    /// * `ram_learning_rate` - Watermark adaptation speed for RAM (η_R)
    /// * `cost_g` - Cost of RAM→VRAM transfer (C^G)
    /// * `cost_r` - Cost of Disk→RAM transfer (C^R)  
    /// * `expert_size` - Uniform expert size in bytes
    ///
    /// # Returns
    /// New algorithm instance with watermarks initialized to 0.0
    pub fn new(
        vram_capacity: usize,
        ram_capacity: usize,
        vram_learning_rate: f64,
        ram_learning_rate: f64,
        cost_g: f64,
        cost_r: f64,
        expert_size: usize,
    ) -> Self {
        Self {
            vram_capacity,
            ram_capacity,
            vram_learning_rate,
            ram_learning_rate,
            cost_g,
            cost_r,
            expert_size,
            vram_watermark: 0.0, // Start with permissive thresholds
            ram_watermark: 0.0,  // Will adapt based on actual usage
        }
    }

    /// Create watermark algorithm with model-specific default parameters
    ///
    /// This convenience constructor provides reasonable defaults for different
    /// MoE model architectures. Learning rates and costs are tuned based on
    /// empirical performance on each model type.
    ///
    /// # Arguments
    /// * `model_type` - The MoE model architecture
    /// * `vram_capacity_mb` - VRAM capacity in megabytes
    /// * `ram_capacity_mb` - RAM capacity in megabytes
    ///
    /// # Default Parameters by Model
    /// - **GPT-OSS-20B**: η=0.01 (balanced adaptation)
    /// - **GPT-OSS-120B**: η=0.005 (slower adaptation for stability)
    /// - **Phi-Tiny-MoE**: η=0.01 (fast adaptation for small model)
    ///
    /// Cost ratios: C^G=1.0, C^R=10.0 (reflects 10x latency difference)
    /// Expert size: 1MB default (typical for MLP layers)
    pub fn from_model(
        model_type: ModelType,
        vram_capacity_mb: usize,
        ram_capacity_mb: usize,
    ) -> Self {
        // Model-specific learning rate tuning based on empirical results
        let (vram_lr, ram_lr) = match model_type {
            ModelType::GptOss20B => (0.01, 0.01),    // Balanced adaptation
            ModelType::GptOss120B => (0.005, 0.005), // Slower for stability
            ModelType::PhiTinyMoe => (0.01, 0.01),   // Fast for small model
        };

        Self::new(
            vram_capacity_mb * 1024 * 1024, // Convert MB to bytes
            ram_capacity_mb * 1024 * 1024,  // Convert MB to bytes
            vram_lr,
            ram_lr,
            1.0,         // cost_g: RAM→VRAM baseline cost
            10.0,        // cost_r: Disk→RAM is ~10x slower than RAM→VRAM
            1024 * 1024, // expert_size: 1MB typical MLP layer size
        )
    }

    /// Make cache placement decisions based on fused probabilities
    ///
    /// This is the core decision-making function of the watermark algorithm.
    /// It computes benefit densities for each expert and compares them against
    /// current watermark thresholds to determine optimal tier placement.
    ///
    /// # Algorithm Steps
    /// 1. **Extract Probabilities**: Get fused activation probabilities for each expert
    /// 2. **Compute Benefits**: Calculate value-per-byte for VRAM and RAM tiers  
    /// 3. **Apply Thresholds**: Compare benefits against current watermarks
    /// 4. **Assign Tiers**: Place experts in highest qualifying tier
    ///
    /// # Mathematical Details
    /// For each expert (e,ℓ):
    /// ```
    /// p = p^{fuse}_{e,ℓ}  // Fused activation probability [0,1]
    ///
    /// // Benefit densities (value per byte)
    /// b^G = p × C^G / S   // VRAM benefit density
    /// b^R = p × C^R / S   // RAM benefit density
    ///
    /// // Tier assignment logic
    /// if b^G ≥ λ_G then tier = VRAM
    /// else if b^R ≥ λ_R then tier = RAM  
    /// else tier = DISK
    /// ```
    ///
    /// # Arguments  
    /// * `fused_prob` - Matrix of fused activation probabilities [layers×experts]
    ///
    /// # Returns
    /// Complete expert state matrix with tier assignments for all experts
    ///
    /// # Performance
    /// Time complexity: O(L × E) where L=layers, E=experts per layer
    /// Space complexity: O(L × E) for result matrix
    pub fn make_cache_decisions(&self, fused_prob: &crate::ExpertProbability) -> ExpertState {
        // Pre-allocate result matrix with exact dimensions
        let num_layers = fused_prob.inner.len();
        let num_experts = if num_layers > 0 {
            fused_prob.inner[0].len()
        } else {
            panic!("No layers in fused probabilities")
        };
        let mut inner = Vec::with_capacity(num_layers);

        // Precompute constants to avoid repeated calculations
        let size = self.expert_size as f64;
        let vram_factor = self.cost_g / size; // For b^G calculation
        let ram_factor = self.cost_r / size; // For b^R calculation

        // Simple nested loops for clear control flow
        for layer_probs in &fused_prob.inner {
            let mut layer_decisions = Vec::with_capacity(num_experts);

            for prob_opt in layer_probs {
                // Extract probability value, treating None as 0.0 (inactive)
                let prob_value = prob_opt.unwrap_or(0.0);

                // Calculate benefit densities for both tiers
                let vram_benefit = prob_value * vram_factor; // b^G
                let ram_benefit = prob_value * ram_factor; // b^R

                // Apply watermark-based tier assignment
                // Higher tiers are preferred when benefits exceed thresholds
                let memory_tier = if vram_benefit >= self.vram_watermark {
                    MemoryTier::Vram // Highest priority: fast access
                } else if ram_benefit >= self.ram_watermark {
                    MemoryTier::Ram // Medium priority: moderate access
                } else {
                    MemoryTier::Disk // Lowest priority: slow access
                };

                layer_decisions.push(memory_tier);
            }

            inner.push(layer_decisions);
        }

        ExpertState { inner }
    }

    /// Update watermarks using subgradient method with feasibility repair
    ///
    /// This is the core adaptive mechanism of the watermark algorithm. It implements
    /// the mathematical watermark evolution described in the research documentation,
    /// combined with a feasibility repair loop to ensure capacity constraints.
    ///
    /// # Mathematical Foundation
    ///
    /// ## Subgradient Method
    /// Watermarks evolve according to projected subgradient descent:
    /// ```
    /// λ_G^{new} = [λ_G + η_G × (usage_G - capacity_G)]₊
    /// λ_R^{new} = [λ_R + η_R × (usage_R - capacity_R)]₊  
    /// ```
    /// Where `[z]₊ = max(z, 0)` ensures non-negative watermarks.
    ///
    /// ## Feasibility Repair Loop
    /// The algorithm iterates until capacity constraints are satisfied:
    /// 1. **Make Decisions**: Apply current watermarks to get expert placements
    /// 2. **Calculate Usage**: Sum memory consumption from decisions
    /// 3. **Update Watermarks**: Apply subgradient formula
    /// 4. **Check Feasibility**: Exit if usage ≤ capacity for both tiers
    /// 5. **Continue**: Otherwise, repeat with updated watermarks
    ///
    /// # Convergence Guarantees
    ///
    /// Mathematical analysis proves this procedure converges:
    /// - **Monotonicity**: Watermarks increase when over-capacity
    /// - **Boundedness**: Benefit densities are finite, so feasible solutions exist
    /// - **Progress**: Each iteration reduces constraint violation
    /// - **Termination**: Algorithm reaches feasible solution in finite iterations
    ///
    /// # Implementation Notes
    ///
    /// - No maximum iteration limit needed (convergence is guaranteed)
    /// - Learning rates control adaptation speed vs stability tradeoff
    /// - Watermarks can increase significantly during repair (this is expected)  
    /// - Final watermarks reflect true scarcity of memory resources
    ///
    /// # Arguments
    /// * `fused_prob` - Fused activation probabilities from prediction pipeline
    ///
    /// # Side Effects  
    /// * Updates internal watermark thresholds (`vram_watermark`, `ram_watermark`)
    /// * May require multiple iterations for convergence
    /// * Watermarks monotonically increase until constraints are satisfied
    pub fn update_watermarks(&mut self, fused_prob: &crate::ExpertProbability) {
        // PHASE 1: VRAM Feasibility Repair Loop
        // Adjust VRAM watermark until VRAM usage is within capacity
        loop {
            let expert_state = self.make_cache_decisions(fused_prob);
            let (vram_usage, _) = self.calculate_tier_usage(&expert_state);

            // VRAM watermark update: λ_G ← [λ_G + η_G(usage_G - K_G)]₊
            let new_vram_watermark = (self.vram_watermark
                + self.vram_learning_rate * (vram_usage as f64 - self.vram_capacity as f64))
                .max(0.0);

            self.vram_watermark = new_vram_watermark;

            // Check VRAM feasibility
            if vram_usage <= self.vram_capacity {
                break; // VRAM constraint satisfied, move to RAM phase
            }
        }

        // PHASE 2: RAM Feasibility Repair Loop
        // Adjust RAM watermark until RAM usage is within capacity
        // VRAM decisions are already feasible from Phase 1
        loop {
            let expert_state = self.make_cache_decisions(fused_prob);
            let (_, ram_usage) = self.calculate_tier_usage(&expert_state);

            // RAM watermark update: λ_R ← [λ_R + η_R(usage_R - K_R)]₊
            let new_ram_watermark = (self.ram_watermark
                + self.ram_learning_rate * (ram_usage as f64 - self.ram_capacity as f64))
                .max(0.0);

            self.ram_watermark = new_ram_watermark;

            // Check RAM feasibility
            if ram_usage <= self.ram_capacity {
                break; // Both VRAM and RAM constraints satisfied
            }
        }
    }

    /// Calculate total memory usage for each tier from expert state
    ///
    /// This function computes the total memory consumption that would result
    /// from a given expert placement decision. It properly accounts for the
    /// containment constraint in the memory hierarchy.
    ///
    /// # Memory Hierarchy Containment Constraint
    ///
    /// The memory system enforces a containment relationship:
    /// **VRAM ⊆ RAM ⊆ DISK**
    ///
    /// This means:
    /// - Every expert in VRAM must also be present in RAM
    /// - Every expert in RAM must also be available on DISK
    /// - Experts on DISK only don't consume cache memory
    ///
    /// # Memory Accounting
    ///
    /// ```
    /// For each expert (e,ℓ):
    ///   match tier_assignment:
    ///     VRAM => { vram_usage += S; ram_usage += S }  // Double counting!
    ///     RAM  => { ram_usage += S }                   // Only RAM
    ///     DISK => { }                                  // No cache usage  
    /// ```
    ///
    /// # Arguments
    /// * `expert_state` - Complete tier assignment matrix for all experts
    ///
    /// # Returns
    /// Tuple `(vram_bytes, ram_bytes)` representing total memory consumption
    ///
    /// # Performance
    /// * Time complexity: O(L × E) - must examine every expert  
    /// * Space complexity: O(1) - only accumulates totals
    ///
    /// # Example
    /// ```rust
    /// // Model with 2 layers, 3 experts per layer
    /// // Expert assignments: [VRAM, RAM, DISK], [RAM, DISK, VRAM]
    /// let (vram, ram) = alg.calculate_tier_usage(&state);
    /// // vram = 2 * expert_size  (2 VRAM experts)
    /// // ram = 4 * expert_size   (2 VRAM + 2 RAM experts)  
    /// ```
    fn calculate_tier_usage(&self, expert_state: &ExpertState) -> (usize, usize) {
        let mut vram_usage = 0; // Total VRAM consumption
        let mut ram_usage = 0; // Total RAM consumption

        // Iterate through all layers and experts to sum memory usage
        for layer_experts in &expert_state.inner {
            for &tier in layer_experts {
                match tier {
                    MemoryTier::Vram => {
                        // VRAM experts consume space in both VRAM and RAM
                        vram_usage += self.expert_size;
                        ram_usage += self.expert_size; // Containment constraint
                    }
                    MemoryTier::Ram => {
                        // RAM experts only consume RAM space
                        ram_usage += self.expert_size;
                    }
                    MemoryTier::Disk => {
                        // DISK experts don't consume cache memory
                        // (they remain on persistent storage)
                    }
                }
            }
        }

        (vram_usage, ram_usage)
    }
}
